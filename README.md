<table>
<tr>
  <td></td>
  <td>クライントも繋がらないノード障害</td>
  <td>クライアントは繋がるノード間障害</td>
</tr>
<tr>
  <td>MySQLのslave(master-slave&sharding)</td>
  <td>C型。故障検知してサービスアウトして他のslaveに任せるロジックがあれば短時間で復旧できる。</td>
  <td>レプリ遅延やレプリが切れた場合：A型。レプリを何とかして追いつかせることでCを復旧させる。</td>
</tr>
<tr>
  <td>MySQLのmaster(master-slave&sharding)</td>
  <td>C型。但し故障検知してフェールオーバーさせるロジックがあれば短時間で復旧できる。</td>
  <td>(ちょっと違うけど)複数masterへのcommit時に片方のみ失敗：A型。完全な一貫性保持は難しい。</td>
</td>
</tr>
<tr>
  <td>MySQL Cluster</td>
  <td>TODO</td>
  <td>TODO</td>
</td>
</tr>
<tr>
  <td>memcached</td>
  <td>データロスト。オリジンが別にあれば故障ノードを外して再度キャッシュさせることでC型っぽい挙動。</td>
  <td>ノード間通信が存在しないので起こり得ない</td>
</tr>
<tr>
  <td>Cassandra</td>
  <td>A型。どれかのノードに到達できればレプリカの存在する複数ノードを使って読み書きできる。一貫性は後で復旧させればよいという発想だが、実際は結構大変らしい。</td>
  <td>A型。上と同じ感じ。一貫性の強度をquorumにするといくつかのレプリカノードから情報を確認して不整合あればそこで復旧させたりもできる。</td>
</tr>
<tr>
  <td>HBaseのRegionServer</td>
  <td>C型。データ自体はHDFSでレプリカがあるのでマスターノードが新しいRSの割り当てを行ったら復旧する。</td>
  <td>C型。クラスタ分断の場合は、少数の側が自殺することでCを保つらしい。死んだ後は多分上と同じ。</td>
</tr>
<tr>
  <td>HBaseのMasterServer</td>
  <td>A型。RSの調停ができなくなるだけなので、早めに復旧できればいいっぽい。スタンバイへのバックアップとかは不明。</td>
  <td>A型。左と同じ。</td>
</tr>
<tr>
  <td>MongoDBのprimary(ReplicaSet)</td>
  <td>C型だけどほぼAもカバー。primary障害の場合、どれかが昇格。クライアント側はどれかにつながりさえすれば今のprimaryを知ることができる。</td>
  <td>primaryと他のreplicaが通信できなくなった場合：多分primary障害と判断されて左と同じ？
</td>
</tr>
<tr>
  <td>ZooKeeper</td>
  <td>A型だけどほぼCもカバー。読み込みはどのノードでも可能だが、結果整合性。primaryの場合failoverの間は書けない(多分)。</td>
  <td>A型？分断した場合どうなるの？</td>
</tr>
</table>